{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ACVSTUaTuUMK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch over to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "DegyO7pexFfc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in datasets\n",
        "\n",
        "# trainData = pd.read_csv('interactions_train.csv')\n",
        "# validationData = pd.read_csv('interactions_validation.csv')\n",
        "# testData = pd.read_csv('interactions_test.csv')"
      ],
      "metadata": {
        "id": "su2_2RrMvCVh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataset to tensors\n",
        "class InteractionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.users = torch.tensor(df['u'].values, dtype=torch.long)\n",
        "        self.recipes = torch.tensor(df['i'].values, dtype=torch.long)\n",
        "        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.recipes[idx], self.ratings[idx]"
      ],
      "metadata": {
        "id": "jwllHfWivHdi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataset to InteractionDataset\n",
        "trainInteractionDataset = InteractionDataset(trainData)\n",
        "valInteractionDataset = InteractionDataset(valData)\n",
        "testInteractionDataset = InteractionDataset(testData)"
      ],
      "metadata": {
        "id": "P6RMqut0wop6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup DataLoaders for training, validating and testing\n",
        "def setupLoaders(batchSize, trainInteractionDataset, valInteractionDataset, testInteractionDataset):\n",
        "    trainLoader = DataLoader(trainInteractionDataset, batch_size=batchSize, shuffle=True)\n",
        "    valLoader = DataLoader(valInteractionDataset, batch_size=batchSize, shuffle=False)\n",
        "    testLoader = DataLoader(testInteractionDataset, batch_size=batchSize, shuffle=False)\n",
        "\n",
        "    return trainLoader, valLoader, testLoader"
      ],
      "metadata": {
        "id": "HhTQxPf3w06M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network\n",
        "class BasicNN(nn.Module):\n",
        "    def __init__(self, n_users, n_recipes, n_factors=10, hidden_size=20):\n",
        "        super(BasicNN, self).__init__()\n",
        "\n",
        "        # Embedding layers\n",
        "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
        "        self.recipe_factors = nn.Embedding(n_recipes, n_factors)\n",
        "\n",
        "        # Dense layers\n",
        "        self.fc1 = nn.Linear(n_factors * 2, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        # Initializing embeddings\n",
        "        nn.init.xavier_uniform_(self.user_factors.weight)\n",
        "        nn.init.xavier_uniform_(self.recipe_factors.weight)\n",
        "\n",
        "    def forward(self, user, recipe):\n",
        "        user_embedding = self.user_factors(user)\n",
        "        recipe_embedding = self.recipe_factors(recipe)\n",
        "\n",
        "        # Concatenate the embeddings\n",
        "        x = torch.cat([user_embedding, recipe_embedding], dim=1)\n",
        "\n",
        "        # Pass through dense layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x.squeeze()\n",
        "\n",
        "class DeepNN(nn.Module):\n",
        "    def __init__(self, n_users, n_recipes, n_factors=20, hidden_sizes=[100, 80, 60, 40, 20], dropout_rate=0.5):\n",
        "        super(DeepNN, self).__init__()\n",
        "\n",
        "        # Embedding layers\n",
        "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
        "        self.recipe_factors = nn.Embedding(n_recipes, n_factors)\n",
        "\n",
        "        # Initializing embeddings\n",
        "        nn.init.xavier_uniform_(self.user_factors.weight)\n",
        "        nn.init.xavier_uniform_(self.recipe_factors.weight)\n",
        "\n",
        "        # Dense layers\n",
        "        layer_sizes = [n_factors * 2] + hidden_sizes + [1]\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
        "            if i < len(layer_sizes) - 2:  # no activation, dropout, or normalization after the last layer\n",
        "                self.layers.append(nn.ReLU())\n",
        "                self.layers.append(nn.Dropout(dropout_rate))\n",
        "                self.layers.append(nn.BatchNorm1d(layer_sizes[i + 1]))\n",
        "\n",
        "    def forward(self, user, recipe):\n",
        "        user_embedding = self.user_factors(user)\n",
        "        recipe_embedding = self.recipe_factors(recipe)\n",
        "\n",
        "        # Concatenate the embeddings\n",
        "        x = torch.cat([user_embedding, recipe_embedding], dim=1)\n",
        "\n",
        "        # Pass through dense layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x.squeeze()"
      ],
      "metadata": {
        "id": "zp0e6rKEw5JF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Max number of Users and Recipe IDs\n",
        "maxUsersId = max(trainData['u'].max(), valData['u'].max(), testData['u'].max())\n",
        "maxRecipesId = max(trainData['i'].max(), valData['i'].max(), testData['i'].max())\n",
        "maxUsersId, maxRecipesId"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_zpEBvQy_21",
        "outputId": "c3694255-656d-4612-fd94-081c38de2e48"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25075, 178264)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trainEpoch(model, trainLoader, device, criterion, optimizer, displayProgression):\n",
        "    model.train()\n",
        "    trainLoss = 0.0\n",
        "    threshold = 4.0\n",
        "    TP_Train = 0\n",
        "    FP_Train = 0\n",
        "    TN_Train = 0\n",
        "    FN_Train = 0\n",
        "    for batch, (users, recipes, ratings) in enumerate(trainLoader):\n",
        "        users = users.to(device)\n",
        "        recipes = recipes.to(device)\n",
        "        ratings = ratings.to(device)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        predictions = model(users, recipes)\n",
        "        loss = criterion(predictions, ratings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        trainLoss += loss.item()\n",
        "\n",
        "        predictedPositives = (predictions >= threshold).float()\n",
        "        actualPositives = (ratings >= threshold).float()\n",
        "        actualNegatives = (ratings < threshold).float()\n",
        "\n",
        "        TP_Train += (predictedPositives * actualPositives).sum().item()\n",
        "        FP_Train += (predictedPositives * actualNegatives).sum().item()\n",
        "        TN_Train += ((1 - predictedPositives) * actualNegatives).sum().item()\n",
        "        FN_Train += ((1 - predictedPositives) * actualPositives).sum().item()\n",
        "\n",
        "        if displayProgression and batch % (len(trainLoader) // 10) == 0 and batch < 10 * (len(trainLoader) // 10):\n",
        "            trainAccuracy = (TP_Train + TN_Train) / (TP_Train + TN_Train + FP_Train + FN_Train)\n",
        "            trainPrecision = (TP_Train / (TP_Train + FP_Train)) if (TP_Train + FP_Train) != 0 else 0\n",
        "            current = (batch + 1) * len(ratings)\n",
        "            print(f\"Training Loss: {loss.item():>.7f} [{current:>6d}/{(len(trainLoader.dataset)):>6d}] Accuracy: {(100*trainAccuracy):>3.1f} Precision: {(100*trainPrecision):>3.1f}\")\n",
        "\n",
        "    trainAccuracy = (TP_Train + TN_Train) / (TP_Train + TN_Train + FP_Train + FN_Train)\n",
        "    trainPrecision = (TP_Train / (TP_Train + FP_Train)) if (TP_Train + FP_Train) != 0 else 0\n",
        "    trainLoss /= len(trainLoader)\n",
        "    return trainLoss, trainAccuracy, trainPrecision"
      ],
      "metadata": {
        "id": "Conznbl-QIxS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testEpoch(model, trainLoader, device, criterion, displayProgression):\n",
        "    model.eval()\n",
        "    valLoss = 0.0\n",
        "    threshold = 4.0\n",
        "    TP_Val = 0\n",
        "    FP_Val = 0\n",
        "    TN_Val = 0\n",
        "    FN_Val = 0\n",
        "    if displayProgression:\n",
        "        print()\n",
        "    with torch.no_grad():\n",
        "        for batch, (users, recipes, ratings) in enumerate(valLoader):\n",
        "            users = users.to(device)\n",
        "            recipes = recipes.to(device)\n",
        "            ratings = ratings.to(device)\n",
        "\n",
        "            predictions = model(users, recipes)\n",
        "            loss = criterion(predictions, ratings)\n",
        "            valLoss += loss.item()\n",
        "\n",
        "            predictedPositives = (predictions >= threshold).float()\n",
        "            actualPositives = (ratings >= threshold).float()\n",
        "            actualNegatives = (ratings < threshold).float()\n",
        "\n",
        "            TP_Val += (predictedPositives * actualPositives).sum().item()\n",
        "            FP_Val += (predictedPositives * actualNegatives).sum().item()\n",
        "            TN_Val += ((1 - predictedPositives) * actualNegatives).sum().item()\n",
        "            FN_Val += ((1 - predictedPositives) * actualPositives).sum().item()\n",
        "\n",
        "            if displayProgression and batch % (len(valLoader) // 10) == 0:\n",
        "                valAccuracy = (TP_Val + TN_Val) / (TP_Val + TN_Val + FP_Val + FN_Val)\n",
        "                valPrecision = (TP_Val / (TP_Val + FP_Val)) if (TP_Val+ FP_Val) != 0 else 0\n",
        "                current = (batch + 1) * len(ratings)\n",
        "                print(f\"Validation Loss: {loss.item():>7f} [{current:>6d}/{(len(valLoader.dataset)):>6d}] Accuracy: {(100*valAccuracy):>3.1f} Precision: {(100*valPrecision):>3.1f}\")\n",
        "\n",
        "    valAccuracy = (TP_Val + TN_Val) / (TP_Val + TN_Val + FP_Val + FN_Val)\n",
        "    valPrecision = (TP_Val / (TP_Val + FP_Val)) if (TP_Val+ FP_Val) != 0 else 0\n",
        "    valLoss /= len(valLoader)\n",
        "    return valLoss, valAccuracy, valPrecision"
      ],
      "metadata": {
        "id": "tZDeC_pQQIn1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = DeepNN(maxUsersId + 1, maxRecipesId + 1)\n",
        "model = model.to(device)\n",
        "criterion = nn.HuberLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "batchSize = 32\n",
        "trainLoader, valLoader, testLoader = setupLoaders(batchSize, trainInteractionDataset, valInteractionDataset, testInteractionDataset)\n",
        "\n",
        "# Training\n",
        "epochs = 10\n",
        "trainLosses = []\n",
        "valLosses = []\n",
        "bestValAccurracy = 0.0\n",
        "displayProgression = True\n",
        "\n",
        "startTime = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------------------------------------------------\")\n",
        "    trainLoss, trainAccuracy, trainPrecision = trainEpoch(model, trainLoader, device, criterion, optimizer, displayProgression)\n",
        "    valLoss, valAccuracy, valPrecision = testEpoch(model, valLoader, device, criterion, displayProgression)\n",
        "\n",
        "    if displayProgression:\n",
        "        print()\n",
        "    print(f\"Training Loss: {trainLoss:.4f}, Accuracy: {(100*trainAccuracy):>3.1f} Precision: {(100*trainPrecision):>3.1f} Validation Loss: {valLoss:.4f}, Accuracy: {(100*valAccuracy):>3.1f}% Precision: {(100*valPrecision):>3.1f}%\\n\")\n",
        "\n",
        "    if valAccuracy < bestValAccurracy:\n",
        "        print(\"Early stopping due to decrease in validation accuracy!\")\n",
        "        break\n",
        "    else:\n",
        "        bestValAccurracy = valAccuracy\n",
        "\n",
        "    scheduler.step(valLoss)\n",
        "\n",
        "endTime = time.time()\n",
        "\n",
        "print(f\"Time: {(endTime-startTime):>.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMpUlQL83AhR",
        "outputId": "3eb43242-6232-46ef-b331-b65d5f58fcf5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------------------------------------------------\n",
            "Training Loss: 3.9234271 [    32/698901] Accuracy: 9.4 Precision: 0.0\n",
            "Training Loss: 0.0776309 [ 69920/698901] Accuracy: 74.9 Precision: 92.1\n",
            "Training Loss: 0.3202780 [139808/698901] Accuracy: 83.7 Precision: 92.3\n",
            "Training Loss: 0.5899310 [209696/698901] Accuracy: 86.6 Precision: 92.4\n",
            "Training Loss: 0.3090078 [279584/698901] Accuracy: 88.1 Precision: 92.4\n",
            "Training Loss: 0.2407153 [349472/698901] Accuracy: 89.0 Precision: 92.4\n",
            "Training Loss: 0.3171371 [419360/698901] Accuracy: 89.6 Precision: 92.4\n",
            "Training Loss: 0.3349264 [489248/698901] Accuracy: 90.0 Precision: 92.4\n",
            "Training Loss: 0.2423636 [559136/698901] Accuracy: 90.3 Precision: 92.4\n",
            "Training Loss: 0.2061665 [629024/698901] Accuracy: 90.5 Precision: 92.4\n",
            "\n",
            "Validation Loss: 0.159120 [    32/  7023] Accuracy: 93.8 Precision: 93.8\n",
            "Validation Loss: 0.315105 [   736/  7023] Accuracy: 87.1 Precision: 87.1\n",
            "Validation Loss: 0.340136 [  1440/  7023] Accuracy: 85.8 Precision: 85.8\n",
            "Validation Loss: 0.288665 [  2144/  7023] Accuracy: 85.7 Precision: 85.7\n",
            "Validation Loss: 0.613332 [  2848/  7023] Accuracy: 85.5 Precision: 85.5\n",
            "Validation Loss: 0.331496 [  3552/  7023] Accuracy: 85.5 Precision: 85.5\n",
            "Validation Loss: 0.673684 [  4256/  7023] Accuracy: 85.3 Precision: 85.3\n",
            "Validation Loss: 0.560716 [  4960/  7023] Accuracy: 85.2 Precision: 85.2\n",
            "Validation Loss: 0.423095 [  5664/  7023] Accuracy: 84.8 Precision: 84.8\n",
            "Validation Loss: 0.509156 [  6368/  7023] Accuracy: 84.3 Precision: 84.3\n",
            "\n",
            "Training Loss: 0.3028, Accuracy: 90.7 Precision: 92.4 Validation Loss: 0.4806, Accuracy: 84.0% Precision: 84.0%\n",
            "\n",
            "Epoch 2\n",
            "-------------------------------------------------------------------------\n",
            "Training Loss: 0.1439808 [    32/698901] Accuracy: 93.8 Precision: 93.8\n",
            "Training Loss: 0.3250826 [ 69920/698901] Accuracy: 92.5 Precision: 92.6\n",
            "Training Loss: 0.0911468 [139808/698901] Accuracy: 92.4 Precision: 92.5\n",
            "Training Loss: 0.2017502 [209696/698901] Accuracy: 92.4 Precision: 92.4\n",
            "Training Loss: 0.2697507 [279584/698901] Accuracy: 92.4 Precision: 92.5\n",
            "Training Loss: 0.0787725 [349472/698901] Accuracy: 92.4 Precision: 92.4\n",
            "Training Loss: 0.4271398 [419360/698901] Accuracy: 92.4 Precision: 92.4\n",
            "Training Loss: 0.4246071 [489248/698901] Accuracy: 92.4 Precision: 92.4\n",
            "Training Loss: 0.2587421 [559136/698901] Accuracy: 92.4 Precision: 92.4\n",
            "Training Loss: 0.0778846 [629024/698901] Accuracy: 92.4 Precision: 92.4\n",
            "\n",
            "Validation Loss: 0.148183 [    32/  7023] Accuracy: 93.8 Precision: 93.8\n",
            "Validation Loss: 0.317613 [   736/  7023] Accuracy: 86.7 Precision: 87.0\n",
            "Validation Loss: 0.331680 [  1440/  7023] Accuracy: 85.6 Precision: 85.8\n",
            "Validation Loss: 0.282718 [  2144/  7023] Accuracy: 85.4 Precision: 85.7\n",
            "Validation Loss: 0.598655 [  2848/  7023] Accuracy: 85.2 Precision: 85.5\n",
            "Validation Loss: 0.329529 [  3552/  7023] Accuracy: 85.3 Precision: 85.6\n",
            "Validation Loss: 0.670699 [  4256/  7023] Accuracy: 85.2 Precision: 85.3\n",
            "Validation Loss: 0.562646 [  4960/  7023] Accuracy: 85.0 Precision: 85.2\n",
            "Validation Loss: 0.415347 [  5664/  7023] Accuracy: 84.7 Precision: 84.8\n",
            "Validation Loss: 0.505772 [  6368/  7023] Accuracy: 84.2 Precision: 84.4\n",
            "\n",
            "Training Loss: 0.2493, Accuracy: 92.4 Precision: 92.4 Validation Loss: 0.4765, Accuracy: 83.9% Precision: 84.0%\n",
            "\n",
            "Early stopping due to decrease in validation accuracy!\n",
            "Time: 235.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testLoss, testAccuracy, testPrecision = testEpoch(model, testLoader, device, criterion, displayProgression)\n",
        "print(f\"Test Loss: {(testLoss):>0.4f}\")\n",
        "print(f\"Test Accuracy: {(100*testAccuracy):>0.1f}%\")\n",
        "print(f\"Test Precision: {(100*testPrecision):>0.1f}%\")"
      ],
      "metadata": {
        "id": "nLWPKmUS3uTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6b1441-7ad6-4bcb-89ae-01e8f8293361"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Loss: 0.148183 [    32/  7023] Accuracy: 93.8 Precision: 93.8\n",
            "Validation Loss: 0.317613 [   736/  7023] Accuracy: 86.7 Precision: 87.0\n",
            "Validation Loss: 0.331680 [  1440/  7023] Accuracy: 85.6 Precision: 85.8\n",
            "Validation Loss: 0.282718 [  2144/  7023] Accuracy: 85.4 Precision: 85.7\n",
            "Validation Loss: 0.598655 [  2848/  7023] Accuracy: 85.2 Precision: 85.5\n",
            "Validation Loss: 0.329529 [  3552/  7023] Accuracy: 85.3 Precision: 85.6\n",
            "Validation Loss: 0.670699 [  4256/  7023] Accuracy: 85.2 Precision: 85.3\n",
            "Validation Loss: 0.562646 [  4960/  7023] Accuracy: 85.0 Precision: 85.2\n",
            "Validation Loss: 0.415347 [  5664/  7023] Accuracy: 84.7 Precision: 84.8\n",
            "Validation Loss: 0.505772 [  6368/  7023] Accuracy: 84.2 Precision: 84.4\n",
            "Test Loss: 0.4765\n",
            "Test Accuracy: 83.9%\n",
            "Test Precision: 84.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saveModel = False\n",
        "if saveModel:\n",
        "    torch.save(model, 'model.pt')"
      ],
      "metadata": {
        "id": "hmEtTmR7Via7"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}
# -*- coding: utf-8 -*-
"""PyTorchRecipeRec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mcRrtLBL62V04VnBzR-wVBKfkxjN4Xdq
"""

import pandas as pd
import numpy as np
import time
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F
from torch.optim.lr_scheduler import ReduceLROnPlateau

# Switch over to GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Read in datasets

# trainData = pd.read_csv('interactions_train.csv')
# validationData = pd.read_csv('interactions_validation.csv')
# testData = pd.read_csv('interactions_test.csv')

# Convert dataset to tensors
class InteractionDataset(Dataset):
    def __init__(self, df):
        self.users = torch.tensor(df['u'].values, dtype=torch.long)
        self.recipes = torch.tensor(df['i'].values, dtype=torch.long)
        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float)

    def __len__(self):
        return len(self.users)

    def __getitem__(self, idx):
        return self.users[idx], self.recipes[idx], self.ratings[idx]

# Convert dataset to InteractionDataset
trainInteractionDataset = InteractionDataset(trainData)
valInteractionDataset = InteractionDataset(valData)
testInteractionDataset = InteractionDataset(testData)

# Setup DataLoaders for training, validating and testing
def setupLoaders(batchSize, trainInteractionDataset, valInteractionDataset, testInteractionDataset):
    trainLoader = DataLoader(trainInteractionDataset, batch_size=batchSize, shuffle=True)
    valLoader = DataLoader(valInteractionDataset, batch_size=batchSize, shuffle=False)
    testLoader = DataLoader(testInteractionDataset, batch_size=batchSize, shuffle=False)

    return trainLoader, valLoader, testLoader

# Neural Network
class BasicNN(nn.Module):
    def __init__(self, n_users, n_recipes, n_factors=10, hidden_size=20):
        super(BasicNN, self).__init__()

        # Embedding layers
        self.user_factors = nn.Embedding(n_users, n_factors)
        self.recipe_factors = nn.Embedding(n_recipes, n_factors)

        # Dense layers
        self.fc1 = nn.Linear(n_factors * 2, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 1)

        # Initializing embeddings
        nn.init.xavier_uniform_(self.user_factors.weight)
        nn.init.xavier_uniform_(self.recipe_factors.weight)

    def forward(self, user, recipe):
        user_embedding = self.user_factors(user)
        recipe_embedding = self.recipe_factors(recipe)

        # Concatenate the embeddings
        x = torch.cat([user_embedding, recipe_embedding], dim=1)

        # Pass through dense layers
        x = F.relu(self.fc1(x))
        x = self.fc2(x)

        return x.squeeze()

class DeepNN(nn.Module):
    def __init__(self, n_users, n_recipes, n_factors=20, hidden_sizes=[100, 80, 60, 40, 20], dropout_rate=0.5):
        super(DeepNN, self).__init__()

        # Embedding layers
        self.user_factors = nn.Embedding(n_users, n_factors)
        self.recipe_factors = nn.Embedding(n_recipes, n_factors)

        # Initializing embeddings
        nn.init.xavier_uniform_(self.user_factors.weight)
        nn.init.xavier_uniform_(self.recipe_factors.weight)

        # Dense layers
        layer_sizes = [n_factors * 2] + hidden_sizes + [1]
        self.layers = nn.ModuleList()
        for i in range(len(layer_sizes) - 1):
            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))
            if i < len(layer_sizes) - 2:  # no activation, dropout, or normalization after the last layer
                self.layers.append(nn.ReLU())
                self.layers.append(nn.Dropout(dropout_rate))
                self.layers.append(nn.BatchNorm1d(layer_sizes[i + 1]))

    def forward(self, user, recipe):
        user_embedding = self.user_factors(user)
        recipe_embedding = self.recipe_factors(recipe)

        # Concatenate the embeddings
        x = torch.cat([user_embedding, recipe_embedding], dim=1)

        # Pass through dense layers
        for layer in self.layers:
            x = layer(x)

        return x.squeeze()

# Max number of Users and Recipe IDs
maxUsersId = max(trainData['u'].max(), valData['u'].max(), testData['u'].max())
maxRecipesId = max(trainData['i'].max(), valData['i'].max(), testData['i'].max())
maxUsersId, maxRecipesId

def trainEpoch(model, trainLoader, device, criterion, optimizer, displayProgression):
    model.train()
    trainLoss = 0.0
    threshold = 4.0
    TP_Train = 0
    FP_Train = 0
    TN_Train = 0
    FN_Train = 0
    for batch, (users, recipes, ratings) in enumerate(trainLoader):
        users = users.to(device)
        recipes = recipes.to(device)
        ratings = ratings.to(device)

        optimizer.zero_grad(set_to_none=True)
        predictions = model(users, recipes)
        loss = criterion(predictions, ratings)
        loss.backward()
        optimizer.step()
        trainLoss += loss.item()

        predictedPositives = (predictions >= threshold).float()
        actualPositives = (ratings >= threshold).float()
        actualNegatives = (ratings < threshold).float()

        TP_Train += (predictedPositives * actualPositives).sum().item()
        FP_Train += (predictedPositives * actualNegatives).sum().item()
        TN_Train += ((1 - predictedPositives) * actualNegatives).sum().item()
        FN_Train += ((1 - predictedPositives) * actualPositives).sum().item()

        if displayProgression and batch % (len(trainLoader) // 10) == 0 and batch < 10 * (len(trainLoader) // 10):
            trainAccuracy = (TP_Train + TN_Train) / (TP_Train + TN_Train + FP_Train + FN_Train)
            trainPrecision = (TP_Train / (TP_Train + FP_Train)) if (TP_Train + FP_Train) != 0 else 0
            current = (batch + 1) * len(ratings)
            print(f"Training Loss: {loss.item():>.7f} [{current:>6d}/{(len(trainLoader.dataset)):>6d}] Accuracy: {(100*trainAccuracy):>3.1f} Precision: {(100*trainPrecision):>3.1f}")

    trainAccuracy = (TP_Train + TN_Train) / (TP_Train + TN_Train + FP_Train + FN_Train)
    trainPrecision = (TP_Train / (TP_Train + FP_Train)) if (TP_Train + FP_Train) != 0 else 0
    trainLoss /= len(trainLoader)
    return trainLoss, trainAccuracy, trainPrecision

def testEpoch(model, trainLoader, device, criterion, displayProgression):
    model.eval()
    valLoss = 0.0
    threshold = 4.0
    TP_Val = 0
    FP_Val = 0
    TN_Val = 0
    FN_Val = 0
    if displayProgression:
        print()
    with torch.no_grad():
        for batch, (users, recipes, ratings) in enumerate(valLoader):
            users = users.to(device)
            recipes = recipes.to(device)
            ratings = ratings.to(device)

            predictions = model(users, recipes)
            loss = criterion(predictions, ratings)
            valLoss += loss.item()

            predictedPositives = (predictions >= threshold).float()
            actualPositives = (ratings >= threshold).float()
            actualNegatives = (ratings < threshold).float()

            TP_Val += (predictedPositives * actualPositives).sum().item()
            FP_Val += (predictedPositives * actualNegatives).sum().item()
            TN_Val += ((1 - predictedPositives) * actualNegatives).sum().item()
            FN_Val += ((1 - predictedPositives) * actualPositives).sum().item()

            if displayProgression and batch % (len(valLoader) // 10) == 0:
                valAccuracy = (TP_Val + TN_Val) / (TP_Val + TN_Val + FP_Val + FN_Val)
                valPrecision = (TP_Val / (TP_Val + FP_Val)) if (TP_Val+ FP_Val) != 0 else 0
                current = (batch + 1) * len(ratings)
                print(f"Validation Loss: {loss.item():>7f} [{current:>6d}/{(len(valLoader.dataset)):>6d}] Accuracy: {(100*valAccuracy):>3.1f} Precision: {(100*valPrecision):>3.1f}")

    valAccuracy = (TP_Val + TN_Val) / (TP_Val + TN_Val + FP_Val + FN_Val)
    valPrecision = (TP_Val / (TP_Val + FP_Val)) if (TP_Val+ FP_Val) != 0 else 0
    valLoss /= len(valLoader)
    return valLoss, valAccuracy, valPrecision

# Initialize the model
model = DeepNN(maxUsersId + 1, maxRecipesId + 1)
model = model.to(device)
criterion = nn.HuberLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5, verbose=True)

batchSize = 32
trainLoader, valLoader, testLoader = setupLoaders(batchSize, trainInteractionDataset, valInteractionDataset, testInteractionDataset)

# Training
epochs = 10
trainLosses = []
valLosses = []
bestValAccurracy = 0.0
displayProgression = True

startTime = time.time()
for epoch in range(epochs):
    print(f"Epoch {epoch+1}\n-------------------------------------------------------------------------")
    trainLoss, trainAccuracy, trainPrecision = trainEpoch(model, trainLoader, device, criterion, optimizer, displayProgression)
    valLoss, valAccuracy, valPrecision = testEpoch(model, valLoader, device, criterion, displayProgression)

    if displayProgression:
        print()
    print(f"Training Loss: {trainLoss:.4f}, Accuracy: {(100*trainAccuracy):>3.1f} Precision: {(100*trainPrecision):>3.1f} Validation Loss: {valLoss:.4f}, Accuracy: {(100*valAccuracy):>3.1f}% Precision: {(100*valPrecision):>3.1f}%\n")

    if valAccuracy < bestValAccurracy:
        print("Early stopping due to decrease in validation accuracy!")
        break
    else:
        bestValAccurracy = valAccuracy

    scheduler.step(valLoss)

endTime = time.time()

print(f"Time: {(endTime-startTime):>.2f}")

testLoss, testAccuracy, testPrecision = testEpoch(model, testLoader, device, criterion, displayProgression)
print(f"Test Loss: {(testLoss):>0.4f}")
print(f"Test Accuracy: {(100*testAccuracy):>0.1f}%")
print(f"Test Precision: {(100*testPrecision):>0.1f}%")

saveModel = False
if saveModel:
    torch.save(model, 'model.pt')